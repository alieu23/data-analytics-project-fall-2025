{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Question 3",
   "id": "15ddfd1bf3ed68ec"
  },
  {
   "cell_type": "markdown",
   "id": "bc725e1bde181d90",
   "metadata": {},
   "source": [
    "## 1. Identify all inconsistent data formats and correct them\n",
    "\n",
    "The first and most visible data quality issue in the dataset appears in the **Timestamp column**, where the same hourly time series is represented using several different string formats. Instead of following a single date-time structure, the dataset mixes multiple conventions for separators, ordering, time notation, and precision. For example, some timestamps follow a day-month-year pattern with AM/PM notation, such as “01-01-2025 12:00AM”, while others use a year-month-day structure with slashes and full seconds, such as “2025/01/01 01:00:00”. Additional rows use a 24-hour clock without seconds (“2025-01-01 02:00”) or the day-month-year format with AM/PM (“01-01-2025 03:00AM”).\n",
    "\n",
    "The dataset also includes European-style entries such as “01/01/2025 09:00” and even ambiguous forms like “02/01/2025 06:00”, where the meaning of “02/01” depends on whether the parser assumes a day-first or month-first convention. Together, these inconsistencies mean that the timestamps cannot be reliably sorted, compared, or resampled until they are standardized into a single coherent datetime format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2b26e7cfa641f9",
   "metadata": {},
   "source": [
    "### Performing EDA before starting data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c6b934ad0f5842c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T01:42:40.646609Z",
     "start_time": "2025-11-24T01:42:40.587054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################## Shape of dataset is:##################  \n",
      " (50, 6)\n",
      "\n",
      "################## Columns of dataset are:##################  \n",
      " ['Timestamp', 'Temperature', 'TempUnit', 'Humidity', 'Airflow', 'StatusText']\n",
      "\n",
      "################## Top 5 rows of dataset are:##################  \n",
      "              Timestamp  Temperature TempUnit  Humidity  Airflow StatusText\n",
      "0   01-01-2025 12:00AM        26.11        C     67.18   127.16         OK\n",
      "1  2025/01/01 01:00:00        85.63        F     51.11   131.22     Sta#us\n",
      "2     2025-01-01 02:00        35.68        C     50.57   141.66         OK\n",
      "3   01-01-2025 03:00AM        84.53        F     58.78   141.08         OK\n",
      "4   01-01-2025 04:00AM        28.37        C     74.34    92.45     Active\n",
      "\n",
      "################## Last 5 rows of dataset are:##################  \n",
      "               Timestamp  Temperature TempUnit  Humidity  Airflow StatusText\n",
      "45  2025/01/03 07:00:00        27.77        C     43.88   113.31     Active\n",
      "46  2025/01/03 08:00:00        87.65        F     63.67   110.50         OK\n",
      "47   01-03-2025 09:00AM        30.39        C     64.58   106.93         OK\n",
      "48     03/01/2025 10:00        32.49        C       NaN   155.31     Sta#us\n",
      "49     03/01/2025 11:00        29.01        C     36.13   128.10    Running\n",
      "\n",
      " Number of missing values: \n",
      " Timestamp      0\n",
      "Temperature    0\n",
      "TempUnit       0\n",
      "Humidity       3\n",
      "Airflow        0\n",
      "StatusText     0\n",
      "dtype: int64\n",
      "\n",
      "##################Statistical Summary of the dataset: ##################\n",
      "                  Timestamp  Temperature TempUnit   Humidity     Airflow  \\\n",
      "count                   50    50.000000       50  47.000000   50.000000   \n",
      "unique                  50          NaN        2        NaN         NaN   \n",
      "top     01-01-2025 12:00AM          NaN        C        NaN         NaN   \n",
      "freq                     1          NaN       27        NaN         NaN   \n",
      "mean                   NaN    55.873800      NaN  54.437660  123.139000   \n",
      "std                    NaN    30.015147      NaN  10.277211   21.862531   \n",
      "min                    NaN    24.610000      NaN  26.210000   79.500000   \n",
      "25%                    NaN    28.747500      NaN  49.190000  106.940000   \n",
      "50%                    NaN    36.005000      NaN  55.630000  125.445000   \n",
      "75%                    NaN    81.125000      NaN  61.975000  133.695000   \n",
      "max                    NaN   132.770000      NaN  74.340000  197.050000   \n",
      "\n",
      "       StatusText  \n",
      "count          50  \n",
      "unique          6  \n",
      "top            OK  \n",
      "freq           15  \n",
      "mean          NaN  \n",
      "std           NaN  \n",
      "min           NaN  \n",
      "25%           NaN  \n",
      "50%           NaN  \n",
      "75%           NaN  \n",
      "max           NaN  \n",
      "\n",
      "################## Dataframe Summary: ##################\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 6 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   Age                   1000 non-null   float64\n",
      " 1   Income                1000 non-null   float64\n",
      " 2   SpendingScore         1000 non-null   float64\n",
      " 3   NumPurchases          1000 non-null   float64\n",
      " 4   LoyaltyLevel          1000 non-null   object \n",
      " 5   LoyaltyLevel_Encoded  1000 non-null   int64  \n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 47.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Basic structure fo the dataset\n",
    "file_path = \"../datasets/Dataset_for_Q3.csv\"   # <== Your uploaded file\n",
    "df_q3 = pd.read_csv(file_path)\n",
    "\n",
    "print(\"\\n################## Shape of dataset is:##################  \\n\", df_q3.shape)\n",
    "print(\"\\n################## Columns of dataset are:##################  \\n\", df_q3.columns.tolist())\n",
    "print(\"\\n################## Top 5 rows of dataset are:##################  \\n\", df_q3.head(5))\n",
    "print(\"\\n################## Last 5 rows of dataset are:##################  \\n\", df_q3.tail(5))\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n Number of missing values: \\n\", df_q3.isna().sum())\n",
    "\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n##################Statistical Summary of the dataset: ##################\\n\", df_q3.describe(include=\"all\"))\n",
    "print(\"\\n################## Dataframe Summary: ##################\\n\")\n",
    "df.info()"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T01:56:51.839383Z",
     "start_time": "2025-11-24T01:56:51.788466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------\n",
    "# 1. Load the dataset\n",
    "# ---------------------------------------------------------\n",
    "file_path = \"../datasets/Dataset_for_Q3.csv\"\n",
    "df_q3 = pd.read_csv(file_path)\n",
    "\n",
    "print(\"Original Data Preview:\")\n",
    "print(df_q3.head())\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Multi-format Timestamp Parser & Replacement\n",
    "# ---------------------------------------------------------\n",
    "timestamp_formats = [\n",
    "    \"%d-%m-%Y %I:%M%p\",     # 01-01-2025 12:00AM\n",
    "    \"%Y/%m/%d %H:%M:%S\",    # 2025/01/01 01:00:00\n",
    "    \"%Y-%m-%d %H:%M\",       # 2025-01-01 02:00\n",
    "    \"%d/%m/%Y %H:%M\",       # 01/01/2025 09:00\n",
    "    \"%d-%m-%Y %H:%M\",       # 01-01-2025 04:00\n",
    "]\n",
    "\n",
    "def parse_timestamp(ts):\n",
    "    for fmt in timestamp_formats:\n",
    "        try:\n",
    "            return datetime.strptime(ts, fmt)\n",
    "        except:\n",
    "            continue\n",
    "    return pd.NaT\n",
    "\n",
    "# 1. Parse dates into a temporary object column for sorting\n",
    "df_q3[\"Temp_Date_Obj\"] = df_q3[\"Timestamp\"].apply(parse_timestamp)\n",
    "\n",
    "# 2. Sort the data chronologically\n",
    "df_q3 = df_q3.sort_values(\"Temp_Date_Obj\").reset_index(drop=True)\n",
    "\n",
    "# 3. OVERWRITE the original 'Timestamp' column with the clean string format\n",
    "df_q3[\"Timestamp\"] = df_q3[\"Temp_Date_Obj\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# 4. Remove the temporary object column\n",
    "df_q3.drop(columns=[\"Temp_Date_Obj\"], inplace=True)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Clean StatusText & Replacement\n",
    "# ---------------------------------------------------------\n",
    "# 1. Create a normalized series (Internal use only)\n",
    "status_normalized = (\n",
    "    df_q3[\"StatusText\"]\n",
    "        .astype(str)\n",
    "        .str.normalize(\"NFKD\")\n",
    "        .str.encode(\"ascii\", errors=\"ignore\")\n",
    "        .str.decode(\"utf-8\")\n",
    "        .str.replace(r\"[^A-Za-z]\", \"\", regex=True)\n",
    "        .str.strip()\n",
    "        .str.title()\n",
    ")\n",
    "\n",
    "# 2. Define the mapping\n",
    "status_map = {\n",
    "    \"Ok\": \"OK\",\n",
    "    \"Status\": \"OK\",\n",
    "    \"Staus\": \"OK\",\n",
    "    \"Ruing\": \"Running\",\n",
    "    \"Active\": \"Active\",\n",
    "    \"Running\": \"Running\"\n",
    "}\n",
    "\n",
    "# 3. OVERWRITE the original 'StatusText' column with the mapped values\n",
    "df_q3[\"StatusText\"] = status_normalized.map(status_map)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. Final Preview\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\nFinal Cleaned Dataset (Old columns replaced):\")\n",
    "print(df_q3.head(10))\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 5. Save cleaned dataset\n",
    "# ---------------------------------------------------------\n",
    "output_path = \"../datasets/output/Dataset_for_Q3_cleaned.csv\"\n",
    "df_q3.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nCleaned dataset saved to: {output_path}\")"
   ],
   "id": "405f41b346103f24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Preview:\n",
      "             Timestamp  Temperature TempUnit  Humidity  Airflow StatusText\n",
      "0   01-01-2025 12:00AM        26.11        C     67.18   127.16         OK\n",
      "1  2025/01/01 01:00:00        85.63        F     51.11   131.22     Sta#us\n",
      "2     2025-01-01 02:00        35.68        C     50.57   141.66         OK\n",
      "3   01-01-2025 03:00AM        84.53        F     58.78   141.08         OK\n",
      "4   01-01-2025 04:00AM        28.37        C     74.34    92.45     Active\n",
      "\n",
      "Final Cleaned Dataset (Old columns replaced):\n",
      "             Timestamp  Temperature TempUnit  Humidity  Airflow StatusText\n",
      "0  2025-01-01 00:00:00        26.11        C     67.18   127.16         OK\n",
      "1  2025-01-01 01:00:00        85.63        F     51.11   131.22         OK\n",
      "2  2025-01-01 02:00:00        35.68        C     50.57   141.66         OK\n",
      "3  2025-01-01 03:00:00        84.53        F     58.78   141.08         OK\n",
      "4  2025-01-01 04:00:00        28.37        C     74.34    92.45     Active\n",
      "5  2025-01-01 05:00:00        26.45        C     55.81   101.24         OK\n",
      "6  2025-01-01 06:00:00        26.20        C     50.84   130.30         OK\n",
      "7  2025-01-01 07:00:00        30.84        C     63.58   130.28         OK\n",
      "8  2025-01-01 08:00:00        83.97        F     62.18   130.30    Running\n",
      "9  2025-01-01 09:00:00        26.09        C     45.49   197.05     Active\n",
      "\n",
      "Cleaned dataset saved to: ../datasets/output/Dataset_for_Q3_cleaned.csv\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "213c561780d92392",
   "metadata": {},
   "source": [
    "## 2. Inspecting the Time Sequence and Identifying Missing Hourly Timestamps\n",
    "\n",
    "After standardizing all timestamps in the Timestamp_clean column, the dataset was evaluated for temporal completeness. Since the sensor should report hourly, every hour between the earliest and latest timestamps should appear at least once. To check this, I generated a full hourly timeline and compared it with the actual recorded timestamps.\n",
    "\n",
    "This comparison revealed 1,376 missing hourly readings, indicating significant gaps in reporting. Examples of missing timestamps early in the series include:\n",
    "- 2025-01-01 10:00:00\n",
    "- 2025-01-01 11:00:00\n",
    "- 2025-01-01 12:00:00\n",
    "- 2025-01-01 13:00:00\n",
    "- 2025-01-01 14:00:00\n",
    "\n",
    "These gaps occur within what should be a continuous hourly sequence and must be addressed or flagged before performing downstream time-series analysis or anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc2a5a437b156329",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T01:42:40.959586Z",
     "start_time": "2025-11-24T01:42:40.936136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing hourly timestamps:\n",
      "DatetimeIndex(['2025-01-01 00:00:00', '2025-01-01 01:00:00',\n",
      "               '2025-01-01 02:00:00', '2025-01-01 03:00:00',\n",
      "               '2025-01-01 04:00:00', '2025-01-01 05:00:00',\n",
      "               '2025-01-01 06:00:00', '2025-01-01 07:00:00',\n",
      "               '2025-01-01 08:00:00', '2025-01-01 09:00:00'],\n",
      "              dtype='datetime64[ns]', freq='h')\n"
     ]
    }
   ],
   "source": [
    "# import cleaned dataset\n",
    "\n",
    "clean_data = \"../datasets/output/Dataset_for_Q3_cleaned.csv\"\n",
    "df_q3_cleaned = pd.read_csv(clean_data)\n",
    "\n",
    "# Ensure Timestamp_clean is sorted\n",
    "df_q3_cleaned = df_q3_cleaned.sort_values(\"Timestamp_clean\")\n",
    "\n",
    "# Build the full expected hourly timeline\n",
    "full_range = pd.date_range(\n",
    "    start=df_q3_cleaned[\"Timestamp_clean\"].min(),\n",
    "    end=df_q3_cleaned[\"Timestamp_clean\"].max(),\n",
    "    freq=\"h\"\n",
    ")\n",
    "\n",
    "# Identify missing timestamps\n",
    "missing_timestamps = full_range.difference(df_q3_cleaned[\"Timestamp_clean\"])\n",
    "\n",
    "print(\"Missing hourly timestamps:\")\n",
    "print(missing_timestamps[:10])   # show first 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f34101f9cbe1491",
   "metadata": {},
   "source": [
    "### 3. Identifying Mixed Temperature Units and Standardizing Them\n",
    "The dataset contains temperatures recorded in both Celsius and Fahrenheit, which makes the series inconsistent. To standardize the data, all Fahrenheit values were converted to Celsius using\n",
    "$$\n",
    "T_C = (T_F - 32) \\times \\frac{5}{9}\n",
    "$$\n",
    "This ensures all readings are on the same scale for accurate analysis."
   ]
  },
  {
   "cell_type": "code",
   "id": "2d01fd835aebb03c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T01:58:59.239911Z",
     "start_time": "2025-11-24T01:58:59.209025Z"
    }
   },
   "source": [
    "# Convert Fahrenheit -> Celsius\n",
    "df_q3_cleaned[\"Temperature_C\"] = df_q3_cleaned.apply(\n",
    "    lambda row: (row[\"Temperature\"] - 32) * 5/9 if row[\"TempUnit\"] == \"F\"\n",
    "                else row[\"Temperature\"],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Optional: round for readability\n",
    "df_q3_cleaned[\"Temperature_C\"] = df_q3_cleaned[\"Temperature_C\"].round(2)\n",
    "\n",
    "print(df_q3_cleaned[[\"Temperature\", \"TempUnit\", \"Temperature_C\"]].head(10))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Temperature TempUnit  Temperature_C\n",
      "0        26.11        C          26.11\n",
      "1        85.63        F          29.79\n",
      "2        35.68        C          35.68\n",
      "3        84.53        F          29.18\n",
      "4        28.37        C          28.37\n",
      "5        26.45        C          26.45\n",
      "6        26.20        C          26.20\n",
      "7        30.84        C          30.84\n",
      "8        83.97        F          28.87\n",
      "9        26.09        C          26.09\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "94c7fb0b29a612e9",
   "metadata": {},
   "source": [
    "### 4. Why Mixed Temperature Units Create False Anomaly Signals\n",
    "\n",
    "Mixing Celsius and Fahrenheit values in the same time series creates sudden jumps that are not real temperature changes. For example, a normal reading like 85°F (~29°C) appears as an extreme spike if treated as 85°C. These artificial jumps distort statistical measures, trigger false outliers, and make anomaly-detection models misinterpret normal behavior as faults or overheating. Converting all readings to a single unit prevents these misleading signals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90ac4ef7ed7cf1e",
   "metadata": {},
   "source": [
    "### 5. Fahrenheit value that could be misinterpreted as an extreme outlier\n",
    "\n",
    "A clear example is the reading:\n",
    "\n",
    "132.77 °F at 2025-01-01 23:00\n",
    "\n",
    "If interpreted as Celsius instead of Fahrenheit, it would appear to be an impossible 132.77°C—a massive outlier compared to the normal 25–30°C range.\n",
    "After conversion, however:\n",
    "\n",
    "<p align=\"center\"><b>132.77&nbsp;°F &approx; 55.98&nbsp;°C</b></p>\n",
    "\n",
    "\n",
    "Still high, but not an extreme error—showing why unit mixing can falsely trigger anomaly alerts."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T02:23:17.638998Z",
     "start_time": "2025-11-24T02:23:17.621111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Filter only Fahrenheit readings\n",
    "df_f = df_q3[df_q3[\"TempUnit\"] == \"F\"].copy()\n",
    "\n",
    "# 2. Identify the single Fahrenheit value that is the highest\n",
    "#    (this is the one most likely to be mistaken as a Celsius outlier)\n",
    "outlier_row = df_f.loc[df_f[\"Temperature\"].idxmax()]\n",
    "\n",
    "# 3. Extract the temperature and timestamp\n",
    "temp_f = outlier_row[\"Temperature\"]\n",
    "timestamp = outlier_row[\"Timestamp\"]\n",
    "\n",
    "# 4. Convert to Celsius\n",
    "temp_c = (temp_f - 32) * 5/9\n",
    "\n",
    "print(\"Fahrenheit value most likely to be misinterpreted as a Celsius outlier:\")\n",
    "print(outlier_row)\n",
    "\n",
    "print(f\"\\nCorrect conversion:\")\n",
    "print(f\"{temp_f:.2f} °F → {temp_c:.2f} °C\")\n",
    "print(f\"Timestamp: {timestamp}\")\n"
   ],
   "id": "5f07f0a30ca374d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fahrenheit value most likely to be misinterpreted as a Celsius outlier:\n",
      "Timestamp      2025-01-01 23:00:00\n",
      "Temperature                 132.77\n",
      "TempUnit                         F\n",
      "Humidity                      30.3\n",
      "Airflow                     133.03\n",
      "StatusText                 Running\n",
      "Name: 13, dtype: object\n",
      "\n",
      "Correct conversion:\n",
      "132.77 °F → 55.98 °C\n",
      "Timestamp: 2025-01-01 23:00:00\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "id": "c1a7504ecd28007f",
   "metadata": {},
   "source": [
    "### 6. By inspecting the temperature values, identify three single-timestamp spikes that do not align with normal sensor behavior\n",
    "\n",
    "By reviewing the temperature values (after converting all Fahrenheit readings to Celsius), three clear one-off spikes stand out. These values are far above the surrounding observations and do not follow normal sensor behavior:\n",
    "\n",
    "- 2025-01-01 23:00 — 132.77 °F ≈ 55.98 °C\n",
    "- 2025-01-02 09:00 — 129.65 °F ≈ 54.25 °C\n",
    "- 2025-01-02 23:00 — 65.30 °C\n",
    "\n",
    "Each of these occurs only once and is surrounded by much lower values (~24–32 °C), indicating they are isolated spikes likely caused by sensor error, transient noise, or corrupted readings rather than genuine environmental changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fd83e43d21fb88c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T01:42:41.173012Z",
     "start_time": "2025-11-24T01:42:41.147641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Timestamp_clean  Temperature TempUnit     Temp_C    z_temp\n",
      "13  2025-01-01 23:00:00       132.77        F  55.983333  3.290677\n",
      "22  2025-01-02 09:00:00       129.65        F  54.250000  3.069553\n",
      "36  2025-01-02 23:00:00        65.30        C  65.300000  4.479217\n"
     ]
    }
   ],
   "source": [
    "# Assume df_q3_cleaned already loaded and has Timestamp_clean, Temperature, TempUnit\n",
    "\n",
    "# Convert all temperatures to Celsius\n",
    "def to_c(row):\n",
    "    t = row[\"Temperature\"]\n",
    "    if row[\"TempUnit\"] == \"F\":\n",
    "        return (t - 32) * 5 / 9\n",
    "    return t\n",
    "\n",
    "df_q3_cleaned[\"Temp_C\"] = df_q3_cleaned.apply(to_c, axis=1)\n",
    "\n",
    "# Compute z-scores for temperature in °C\n",
    "mean = df_q3_cleaned[\"Temp_C\"].mean()\n",
    "std = df_q3_cleaned[\"Temp_C\"].std()\n",
    "df_q3_cleaned[\"z_temp\"] = (df_q3_cleaned[\"Temp_C\"] - mean) / std\n",
    "\n",
    "# Pick large, isolated spikes (|z| > 3 as a simple criterion)\n",
    "spikes = df_q3_cleaned.loc[df_q3_cleaned[\"z_temp\"].abs() > 3,\n",
    "                   [\"Timestamp_clean\", \"Temperature\", \"TempUnit\", \"Temp_C\", \"z_temp\"]]\n",
    "\n",
    "print(spikes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d43bd0580dc64b",
   "metadata": {},
   "source": [
    "## 7. Methods for Handling Single-Timestamp Spikes\n",
    "Method 1 – Correct or Replace (Imputation / Smoothing)\n",
    "\n",
    "### Method 1 — Remove/Correct the Spike Using Interpolation\n",
    "\n",
    "Description:\n",
    "When an anomaly is clearly a sensor glitch (e.g., a sudden jump from 27°C to 56°C and back), the spike can be replaced using statistical or time-based interpolation (linear, nearest, rolling median).\n",
    "\n",
    "#### When appropriate:\n",
    "- When the physical system changes gradually (HVAC, environmental monitoring).\n",
    "- When the spike is mathematically impossible or violates operational limits.\n",
    "- When preparing data for machine learning, forecasting, or smoothing.\n",
    "\n",
    "#### Why:\n",
    "It prevents unrealistic spikes from distorting trends, averages, and anomaly models.\n",
    "\n",
    "### Method 2 — Keep the Value but Flag It as an Anomaly\n",
    "\n",
    "Description:\n",
    "Rather than removing the spike, the value is kept, but you add an AnomalyFlag = 1.\n",
    "\n",
    "#### When appropriate:\n",
    "- When the spike might represent a real event, such as overheating, airflow blockage, or equipment failure.\n",
    "- When maintaining data integrity is important (industrial logging, safety systems).\n",
    "- When analysts need to trace the real behavior of the system.\n",
    "\n",
    "#### Why:\n",
    "This preserves potentially meaningful events while still highlighting them for investigation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4793ca49ac4bf910",
   "metadata": {},
   "source": [
    "## 8. Decision for Each Temperature Spike\n",
    "#### Spike 1 — 132.77 °F (≈ 55.98 °C) at 2025-01-01 23:00\n",
    "\n",
    "##### Decision: Correct (or interpolate) for analysis; keep + flag for monitoring.\n",
    "##### Justification:\n",
    "This is an isolated jump far outside the normal 25–30 °C pattern, indicating a probable sensor glitch. For machine-learning or trend analysis, the value should be corrected to avoid biasing the model. However, it should still be retained and flagged in operational logs in case the spike reflects a brief overheating event.\n",
    "\n",
    "#### Spike 2 — 129.65 °F (≈ 54.25 °C) at 2025-01-02 09:00\n",
    "\n",
    "##### Decision: Correct (interpolate).\n",
    "##### Justification:\n",
    "This spike occurs only at a single timestamp and returns immediately to normal. The abrupt jump is inconsistent with physical HVAC behavior, which cannot heat or cool that quickly. Treating it as a faulty reading prevents false anomaly alerts or exaggerated variability in the temperature series.\n",
    "\n",
    "#### Spike 3 — 65.30 °C at 2025-01-02 23:00\n",
    "\n",
    "##### Decision: Remove or correct, depending on system limits.\n",
    "##### Justification:\n",
    "This spike is extreme even in Celsius and does not match the trend before or after. If the monitored equipment cannot reach 65 °C under normal operation, the value should be removed as impossible. If such temperatures are physically possible (e.g., a heater or motor housing), it should be kept but flagged for further investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c4aaf96ce2a79a",
   "metadata": {},
   "source": [
    "## 9. The StatusText column contains corrupted labels\n",
    "The StatusText column contained several corrupted labels, including inconsistent spelling, symbols, and accented characters (e.g., Sta#us, Ru!!ing, Açtive). To standardize these, I first normalized the raw text by removing accents, stripping non-alphabetic characters, and applying consistent capitalization. This produced intermediate forms such as Staus, Ruing, and Active. I then created a mapping table that explicitly grouped these variants into clean categories: both OK and Sta#us/Staus were mapped to “OK”, Active/Açtive to “Active”, and Running/Ru!!ing (Ruing) to “Running”. The result is a StatusClean field that provides consistent, analyzable status labels across the entire dataset.\n",
    "\n",
    "#### This is also handled in 3. Normalization / Scaling — Best Method: Robust Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "671a7c5f29ac55ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T01:42:41.215853Z",
     "start_time": "2025-11-24T01:42:41.192070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   StatusText StatusText_norm StatusClean\n",
      "0          OK              Ok          OK\n",
      "1      Sta#us           Staus          OK\n",
      "2          OK              Ok          OK\n",
      "3          OK              Ok          OK\n",
      "4      Active          Active      Active\n",
      "5      Sta#us           Staus          OK\n",
      "6          OK              Ok          OK\n",
      "7      Sta#us           Staus          OK\n",
      "8     Running         Running     Running\n",
      "9      Active          Active      Active\n",
      "10     Sta#us           Staus          OK\n",
      "11         OK              Ok          OK\n",
      "12         OK              Ok          OK\n",
      "13    Ru!!ing           Ruing     Running\n",
      "14     Active          Active      Active\n",
      "15     Active          Active      Active\n",
      "16    Ru!!ing           Ruing     Running\n",
      "17     Sta#us           Staus          OK\n",
      "18     Active          Active      Active\n",
      "19         OK              Ok          OK\n",
      "20         OK              Ok          OK\n",
      "21    Running         Running     Running\n",
      "22     Active          Active      Active\n",
      "23     Sta#us           Staus          OK\n",
      "24    Ru!!ing           Ruing     Running\n",
      "25    Ru!!ing           Ruing     Running\n",
      "26     Sta#us           Staus          OK\n",
      "27         OK              Ok          OK\n",
      "28     Açtive          Active      Active\n",
      "29    Running         Running     Running\n",
      "30    Ru!!ing           Ruing     Running\n",
      "31     Active          Active      Active\n",
      "32    Running         Running     Running\n",
      "33     Active          Active      Active\n",
      "34         OK              Ok          OK\n",
      "35         OK              Ok          OK\n",
      "36    Running         Running     Running\n",
      "37         OK              Ok          OK\n",
      "38    Running         Running     Running\n",
      "39     Açtive          Active      Active\n",
      "40    Running         Running     Running\n",
      "41     Active          Active      Active\n",
      "42         OK              Ok          OK\n",
      "43     Sta#us           Staus          OK\n",
      "44    Running         Running     Running\n",
      "45         OK              Ok          OK\n",
      "46    Ru!!ing           Ruing     Running\n",
      "47    Running         Running     Running\n",
      "48    Running         Running     Running\n",
      "49         OK              Ok          OK\n"
     ]
    }
   ],
   "source": [
    "# Normalize corrupted labels\n",
    "df_q3_cleaned[\"StatusText_norm\"] = (\n",
    "    df_q3_cleaned[\"StatusText\"]\n",
    "        .astype(str)\n",
    "        .str.normalize(\"NFKD\")\n",
    "        .str.encode(\"ascii\", errors=\"ignore\")\n",
    "        .str.decode(\"utf-8\")\n",
    "        .str.replace(r\"[^A-Za-z]\", \"\", regex=True)\n",
    "        .str.strip()\n",
    "        .str.title()\n",
    ")\n",
    "\n",
    "# Final mapping table\n",
    "# Final mapping table\n",
    "status_map = {\n",
    "    \"Ok\": \"OK\",             # Maps 'OK' -> 'Ok' back to 'OK'\n",
    "    \"Status\": \"OK\",         # Maps 'Status' -> 'OK'\n",
    "    \"Staus\": \"OK\",          # Handles 'Sta#us' -> 'Staus'\n",
    "    \"Ruing\": \"Running\",     # <--- NEW: Handles 'Ru!!ing' -> 'Ruing'\n",
    "    \"Active\": \"Active\",     # Handles 'Açtive' -> 'Active'\n",
    "    \"Running\": \"Running\"\n",
    "}\n",
    "# Apply mapping\n",
    "df_q3_cleaned[\"StatusClean\"] = df_q3_cleaned[\"StatusText_norm\"].map(status_map)\n",
    "\n",
    "print(df_q3_cleaned[[\"StatusText\", \"StatusText_norm\", \"StatusClean\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc102330da0811",
   "metadata": {},
   "source": [
    "## 10. Briefly explain one potential risk of applying incorrect fuzzy text matching in an industrial monitoring or safety system.\n",
    "\n",
    "### Risk: Semantic Misinterpretation Leading to False Negatives\n",
    "\n",
    "Explanation: Fuzzy matching algorithms correct text based on character similarity (Levenshtein distance) without understanding context. In industrial safety, terms with opposite meanings often look similar (e.g., \"Severed\" vs. \"Secured\").\n",
    "\n",
    "Consequence: If a sensor reports a corrupted critical error like \"Se!ured\" (intended to be \"Severed\"), an incorrect fuzzy match might map it to the benign status \"Secured\" or \"OK.\" This False Negative would suppress the alarm, leading operators to believe the system is safe when a critical failure has actually occurred, potentially resulting in catastrophic equipment damage or injury."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6e29e572a998f7",
   "metadata": {},
   "source": [
    "# 11. Identify at least two duplicate airflow readings that appear across different timestamps\n",
    "Identification of Duplicates: Analysis of the Airflow column reveals a specific value, 130.3, which repeats exactly across three distinct timestamps: 2025-01-01 06:00, 01-01-2025 08:00AM, and 2025-01-02 17:00"
   ]
  },
  {
   "cell_type": "code",
   "id": "ba59f6a7e8690602",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T02:26:31.582683Z",
     "start_time": "2025-11-24T02:26:31.558Z"
    }
   },
   "source": [
    "# 2. Find duplicates in the specific column 'Airflow'\n",
    "# keep=False ensures ALL instances of the duplicate are shown, not just the extra ones.\n",
    "\n",
    "df_airflow_duplicate = pd.read_csv(\"../datasets/output/Dataset_for_Q3_cleaned.csv\")\n",
    "duplicates = df_airflow_duplicate[df_airflow_duplicate.duplicated(subset=['Airflow'], keep=False)]\n",
    "\n",
    "# 3. Sort by Airflow to see them grouped together\n",
    "result = duplicates[['Timestamp', 'Airflow']].sort_values(by='Airflow')\n",
    "\n",
    "print(result.head(2))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Timestamp  Airflow\n",
      "6  2025-01-01 06:00:00    130.3\n",
      "8  2025-01-01 08:00:00    130.3\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "id": "73a37ebfab86429f",
   "metadata": {},
   "source": [
    "## 12. Decide whether each repeated value is likely a true repeated measurement or an accidental duplication, and justify your reasoning based on airflow behavior in systems.\n",
    "The repeated airflow value of 130.3 is likely an accidental duplication or data artifact, not a true repeated measurement.\n",
    "\n",
    "### Justification based on Airflow Behavior:\n",
    "- **Continuous Fluctuation:** Airflow in industrial systems is a dynamic physical quantity. It is subject to constant, minute variations caused by fluid turbulence, slight changes in fan speed, and environmental resistance. In a functioning system, readings typically fluctuate by small decimals (e.g., 130.28, 130.32) rather than holding a static value.\n",
    "- **Statistical Improbability:** The sensor reports data with high precision (two decimal places). The probability of a sensor independently measuring the exact same floating-point value (130.30) at three different times—separated by hours and days—is statistically negligible.\n",
    "- **Sensor \"Latching\":** This pattern is characteristic of a sensor or data logger \"freezing,\" where the system fails to capture a new reading and simply repeats the last known valid value (a \"zero-order hold\" error) or defaults to a pre-set error code value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7570423d77c394f",
   "metadata": {},
   "source": [
    "## Data Quality Evaluation\n",
    "The data-quality issues in this IoT time series can easily distort analysis if not corrected. Inconsistent timestamp formats and long gaps in hourly readings break temporal continuity and mislead trend or anomaly detection models.\n",
    "\n",
    "Mixed Celsius and Fahrenheit values are especially harmful because unconverted readings appear as extreme outliers, triggering false alerts. Corrupted status labels and duplicate airflow values can further bias categorical summaries or operational monitoring. Overall, mixed units and irregular timestamps are the most damaging because they alter both the scale and sequence of the signal.\n",
    "\n",
    "Improving future data collection requires enforcing strict schemas, real-time timestamp validation, automated unit checks, and rules that flag impossible values and missing intervals at ingestion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
